{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/text_emotion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n",
       "       'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['sentiment'].unique())  #Number of different emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "text = df['content']\n",
    "labels = df['sentiment']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "text = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "#Setting the sequence length for the padding\n",
    "seq_length = 30\n",
    "\n",
    "sequences = pad_sequences(text, maxlen = seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of the sequences before padding is: 13.5158 words\n",
      "The max length of the sequences before padding is: 37 words\n",
      "The min length of the sequences before padding is: 1 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "agg_list = np.array([len(i) for i in text])\n",
    "avg_length = agg_list.mean()\n",
    "max_length = np.array([len(i) for i in text]).max()\n",
    "min_length = np.array([len(i) for i in text]).min()\n",
    "\n",
    "print(\"The average length of the sequences before padding is: {} words\".format(avg_length))\n",
    "print(\"The max length of the sequences before padding is: {} words\".format(max_length))\n",
    "print(\"The min length of the sequences before padding is: {} words\".format(min_length))\n",
    "\n",
    "plt.hist(agg_list, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the emotion dict to encode the label\n"
     ]
    }
   ],
   "source": [
    "emotion_dict = {}\n",
    "\n",
    "for k in range(len(labels.unique())):\n",
    "    emotion_dict[labels.unique()[k]] = k\n",
    "    \n",
    "print('Creating the emotion dict to encode the label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the encoded target\n",
    "\n",
    "encoded_labels = np.array(list(labels.map(lambda x: emotion_dict[x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(32000, 30) \n",
      "Validation set: \t(8000, 30)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(sequences)*split_frac)\n",
    "train_x, val_x = sequences[:split_idx], sequences[split_idx:]\n",
    "train_y, val_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(\"data/embedding/glove.6B/\", 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "\n",
    "import torch\n",
    "\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, hidden_dim, \n",
    "                 embedding_dim, embedding_dict_size, \n",
    "                 seq_length, output_size):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.n_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_dict_size = embedding_dict_size\n",
    "        self.seq_length = seq_length\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix))\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.embedding_dict_size, self.embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.seq_length, self.hidden_dim, \n",
    "                            self.n_layers, batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_dim, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.view(batch_size,self.embedding_dim,self.seq_length)\n",
    "        lstm_out, hidden = self.lstm(embeds)\n",
    "        lstm_out = lstm_out.contiguous().view(batch_size, self.embedding_dim, self.hidden_dim)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = out[:,-1,:]\n",
    "\n",
    "        \n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "val_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter settting\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "num_layers = 1\n",
    "hidden_dim = 50\n",
    "output_size = len(df['sentiment'].unique())\n",
    "embedding_dim = 200\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "output_size = len(emotion_dict)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "net = Net(num_layers, hidden_dim, embedding_dim, vocab_size, seq_length, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embedding): Embedding(48998, 200)\n",
      "  (lstm): LSTM(30, 50, batch_first=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=50, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 128... Loss: 2.575969... Val Loss: 2.573133\n",
      "Epoch: 1/4... Step: 256... Loss: 2.583178... Val Loss: 2.564871\n",
      "Epoch: 1/4... Step: 384... Loss: 2.553805... Val Loss: 2.557342\n",
      "Epoch: 2/4... Step: 512... Loss: 2.563453... Val Loss: 2.549681\n",
      "Epoch: 2/4... Step: 640... Loss: 2.525927... Val Loss: 2.542375\n",
      "Epoch: 2/4... Step: 768... Loss: 2.535498... Val Loss: 2.535293\n",
      "Epoch: 2/4... Step: 896... Loss: 2.515574... Val Loss: 2.528323\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 4\n",
    "counter = 0\n",
    "clip = 5\n",
    "print_every = 128\n",
    "\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    \n",
    "    h = net.init_hidden(batch_size)\n",
    "    net.train()\n",
    "    \n",
    "    for text_sequences, targets in train_loader:\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        #resetting the gradient\n",
    "        net.zero_grad()\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            text_sequences, targets = text_sequences.cuda(), targets.cuda()\n",
    "            \n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "            \n",
    "        #Not sure why pytorch refuses the indices of text_sequences to be int.32\n",
    "        input_text = text_sequences.type(torch.long)\n",
    "        outputs, h = net(input_text, h) \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        #Clipping gradient to avoid exploding gradients\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if(counter % print_every == 0):\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            \n",
    "            for text_sequences, targets in val_loader:\n",
    "                # Get validation loss\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    text_sequences, targets = text_sequences.cuda(), targets.cuda()\n",
    "                \n",
    "                input_text = text_sequences.type(torch.long)\n",
    "                outputs, val_h = net(input_text, val_h)\n",
    "                val_loss = criterion(outputs, targets)\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, nb_epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('data/lstm_model.pt',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embedding): Embedding(48998, 200)\n",
      "  (lstm): LSTM(30, 50, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=50, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(48998, 200)\n",
       "  (lstm): LSTM(30, 50, num_layers=2, batch_first=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=50, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
