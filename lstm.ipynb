{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/text_emotion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n",
       "       'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['sentiment'].unique())  #Number of different emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of the sequences before padding is: 13.5158 words\n",
      "The max length of the sequences before padding is: 37 words\n",
      "The min length of the sequences before padding is: 1 words\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "agg_list = np.array([len(i) for i in text])\n",
    "avg_length = agg_list.mean()\n",
    "max_length = np.array([len(i) for i in text]).max()\n",
    "min_length = np.array([len(i) for i in text]).min()\n",
    "\n",
    "print(\"The average length of the sequences before padding is: {} words\".format(avg_length))\n",
    "print(\"The max length of the sequences before padding is: {} words\".format(max_length))\n",
    "print(\"The min length of the sequences before padding is: {} words\".format(min_length))\n",
    "\n",
    "plt.hist(agg_list, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "text = df['content']\n",
    "labels = df['sentiment']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "text = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "#Setting the sequence length for the padding\n",
    "seq_length = 30\n",
    "\n",
    "sequences = pad_sequences(text, maxlen = seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the emotion dict to encode the label\n"
     ]
    }
   ],
   "source": [
    "emotion_dict = {}\n",
    "\n",
    "for k in range(len(labels.unique())):\n",
    "    emotion_dict[labels.unique()[k]] = k\n",
    "    \n",
    "print('Creating the emotion dict to encode the label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the encoded target\n",
    "\n",
    "encoded_labels = np.array(list(labels.map(lambda x: emotion_dict[x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(32000, 30) \n",
      "Validation set: \t(8000, 30)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(sequences)*split_frac)\n",
    "train_x, val_x = sequences[:split_idx], sequences[split_idx:]\n",
    "train_y, val_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "\n",
    "import torch\n",
    "\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, hidden_dim, \n",
    "                 embedding_dim, embedding_dict_size, \n",
    "                 seq_length, output_size):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.n_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_dict_size = embedding_dict_size\n",
    "        self.seq_length = seq_length\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.embedding_dict_size, self.embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.seq_length, self.hidden_dim, \n",
    "                            self.n_layers, batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_dim, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.view(batch_size,self.embedding_dim,self.seq_length)\n",
    "        lstm_out, hidden = self.lstm(embeds)\n",
    "        lstm_out = lstm_out.contiguous().view(batch_size, self.embedding_dim, self.hidden_dim)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = out[:,-1,:]\n",
    "\n",
    "        \n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "val_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embedding): Embedding(48998, 200)\n",
      "  (lstm): LSTM(30, 50, batch_first=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=50, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter settting\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "num_layers = 1\n",
    "hidden_dim = 50\n",
    "output_size = len(df['sentiment'].unique())\n",
    "embedding_dim = 200\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "output_size = len(emotion_dict)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "net = Net(num_layers, hidden_dim, embedding_dim, vocab_size, seq_length, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 128... Loss: 2.470800... Val Loss: 2.513041\n",
      "Epoch: 1/4... Step: 256... Loss: 2.528027... Val Loss: 2.506971\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-315-24ffa2b4b828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#Clipping gradient to avoid exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(self, p, dim, keepdim, dtype)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;34mr\"\"\"See :func:`torch.norm`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpstrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrobenius_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"nuc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fro\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_epochs = 4\n",
    "counter = 0\n",
    "clip = 5\n",
    "print_every = 128\n",
    "\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    \n",
    "    h = net.init_hidden(batch_size)\n",
    "    net.train()\n",
    "    \n",
    "    for text_sequences, targets in train_loader:\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        #resetting the gradient\n",
    "        net.zero_grad()\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            text_sequences, targets = text_sequences.cuda(), targets.cuda()\n",
    "            \n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "            \n",
    "        #Not sure why pytorch refuses the indices of text_sequences to be int.32\n",
    "        input_text = text_sequences.type(torch.long)\n",
    "        outputs, h = net(input_text, h) \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        #Clipping gradient to avoid exploding gradients\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if(counter % print_every == 0):\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            \n",
    "            for text_sequences, targets in val_loader:\n",
    "                # Get validation loss\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    text_sequences, targets = text_sequences.cuda(), targets.cuda()\n",
    "                \n",
    "                input_text = text_sequences.type(torch.long)\n",
    "                outputs, val_h = net(input_text, val_h)\n",
    "                val_loss = criterion(outputs, targets)\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, nb_epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
