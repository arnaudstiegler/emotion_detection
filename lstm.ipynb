{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/text_emotion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n",
       "       'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['sentiment'].unique())  #Number of different emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/arnaudstiegler/anaconda3/envs/emotion_detection/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/arnaudstiegler/anaconda3/envs/emotion_detection/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/arnaudstiegler/anaconda3/envs/emotion_detection/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/arnaudstiegler/anaconda3/envs/emotion_detection/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/arnaudstiegler/anaconda3/envs/emotion_detection/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/arnaudstiegler/anaconda3/envs/emotion_detection/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/arnaudstiegler/anaconda3/envs/emotion_detection/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/arnaudstiegler/anaconda3/envs/emotion_detection/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/arnaudstiegler/anaconda3/envs/emotion_detection/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/arnaudstiegler/anaconda3/envs/emotion_detection/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/arnaudstiegler/anaconda3/envs/emotion_detection/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/arnaudstiegler/anaconda3/envs/emotion_detection/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "text = df['content']\n",
    "labels = df['sentiment']\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "text = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "#Setting the sequence length for the padding\n",
    "seq_length = 30\n",
    "\n",
    "sequences = pad_sequences(text, maxlen = seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average length of the sequences before padding is: 13.5158 words\n",
      "The max length of the sequences before padding is: 37 words\n",
      "The min length of the sequences before padding is: 1 words\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "agg_list = np.array([len(i) for i in text])\n",
    "avg_length = agg_list.mean()\n",
    "max_length = np.array([len(i) for i in text]).max()\n",
    "min_length = np.array([len(i) for i in text]).min()\n",
    "\n",
    "print(\"The average length of the sequences before padding is: {} words\".format(avg_length))\n",
    "print(\"The max length of the sequences before padding is: {} words\".format(max_length))\n",
    "print(\"The min length of the sequences before padding is: {} words\".format(min_length))\n",
    "\n",
    "plt.hist(agg_list, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the emotion dict to encode the label\n"
     ]
    }
   ],
   "source": [
    "emotion_dict = {}\n",
    "\n",
    "for k in range(len(labels.unique())):\n",
    "    emotion_dict[labels.unique()[k]] = k\n",
    "    \n",
    "print('Creating the emotion dict to encode the label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the encoded target\n",
    "\n",
    "encoded_labels = np.array(list(labels.map(lambda x: emotion_dict[x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(32000, 30) \n",
      "Validation set: \t(8000, 30)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(sequences)*split_frac)\n",
    "train_x, val_x = sequences[:split_idx], sequences[split_idx:]\n",
    "train_y, val_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(\"data/embedding/\", 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "\n",
    "import torch\n",
    "\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, hidden_dim, \n",
    "                 embedding_dim, embedding_dict_size, \n",
    "                 seq_length, output_size):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        self.n_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_dict_size = embedding_dict_size\n",
    "        self.seq_length = seq_length\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix))\n",
    "    \n",
    "        #self.embedding = nn.Embedding(self.embedding_dict_size, self.embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.seq_length, hidden_size=self.hidden_dim, \n",
    "                            num_layers=self.n_layers, batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_dim, output_size)\n",
    "\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.view(batch_size,self.embedding_dim,self.seq_length).type(torch.float)\n",
    "        lstm_out, hidden = self.lstm(embeds)\n",
    "        lstm_out = lstm_out.contiguous().view(batch_size, self.embedding_dim, self.hidden_dim)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = out[:,-1,:]\n",
    "\n",
    "        \n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "val_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter settting\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "num_layers = 1\n",
    "hidden_dim = 50\n",
    "output_size = len(df['sentiment'].unique())\n",
    "embedding_dim = 300\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "output_size = len(emotion_dict)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "net = Net(num_layers, hidden_dim, embedding_dim, vocab_size, seq_length, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embedding): Embedding(48998, 300)\n",
      "  (lstm): LSTM(30, 50, batch_first=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=50, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 64... Loss: 2.536418... Val Loss: 2.552572 Val Accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb_epochs = 4\n",
    "counter = 0\n",
    "clip = 5\n",
    "print_every = 64\n",
    "\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    \n",
    "    h = net.init_hidden(batch_size)\n",
    "    net.train()\n",
    "    \n",
    "    for text_sequences, targets in train_loader:\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        #resetting the gradient\n",
    "        net.zero_grad()\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            text_sequences, targets = text_sequences.cuda(), targets.cuda()\n",
    "            \n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "            \n",
    "        #Not sure why pytorch refuses the indices of text_sequences to be int.32\n",
    "        input_text = text_sequences.type(torch.long)\n",
    "        outputs, h = net(input_text, h) \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        #Clipping gradient to avoid exploding gradients\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if(counter % print_every == 0):\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            \n",
    "            for text_sequences, targets in val_loader:\n",
    "                # Get validation loss\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    text_sequences, targets = text_sequences.cuda(), targets.cuda()\n",
    "                \n",
    "                input_text = text_sequences.type(torch.long)\n",
    "                outputs, val_h = net(input_text, val_h)\n",
    "                val_loss = criterion(outputs, targets)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "                y_pred = outputs.detach().numpy().argmax(axis=1)\n",
    "                acc = accuracy_score(targets,y_pred)\n",
    "            \n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, nb_epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
    "                 \"Val Accuracy: {:.6f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = torch.load('data/lstm_model.pt',map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embedding): Embedding(48998, 200)\n",
      "  (lstm): LSTM(30, 50, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=50, out_features=13, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(48998, 200)\n",
       "  (lstm): LSTM(30, 50, num_layers=2, batch_first=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc): Linear(in_features=50, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
